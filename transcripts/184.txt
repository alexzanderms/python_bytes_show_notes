00:00:00 Hello, and welcome to Python bytes where we deliver Python news and the headlines directly to your ear buds. This is Episode 184. recorded on May 27 2020. I'm Brian knockin. And I'm Michael Kennedy. And this episode is sponsored by Digital Ocean. Thank you digitalocean. Yeah, thanks, digitalocean really appreciate it. Well, I'm waiting. Michael, you're waiting. What are you waiting for? I'm waiting for the next async IO story. I, I guess it's my turn, isn't it? Okay, sorry. Let me see if I can get this right. So the topic I want to talk about is waiting in async. io. Yeah. So the magic of async. io, which was introduced in Python three, four never really appeared until Python three, five, when the async and await keywords came into being, which let you write code that looks like standard single threaded serial code, but actually is multi threaded, or at least parallel concurrent to some degree depends on how you're running it, whether it's truly multi threaded. Anyway, there's a lot of options, let's say on how you can interact with these co routines and these tasks that are generated by the async. io framework. And anytime there's like four ways to do something in programming, you should be asking yourself, one wire, there are four ways to do this. But more importantly, Windows way one apply best. And what scenario should I use? Way three? And what is the trade off between two and four? and so on? Right? So that's the case with async. io. There's tons of ways to wait or await things. And Hank, Hiddink good pronunciation for me. I know you got it. You got it. That's good. Yeah. Hey,

00:01:47 sorry, that's just a running thing I can never get I know. Well, I'm the problem is I think I had it right. And then we went back and forth. to so many variations. Now I'm, it's broken.

00:01:59 Sorry about mess up your name there. So wrote a great article, though, called waiting and async IO that does exactly that, that says, here's all the ways. Here's the pluses. And here's the minuses and the situations in which you apply them. So if you're like serious about using async IO, and you're building real things, basically, this podcast episode is for you, because I have this and another one that bring two really cool ideas together. But let's talk about the waiting one first. So it's really easy to start doing some work, right, I can have two co routines, let's say F and G. And I could say I want the result of F by saying, you know, result equals await, calling F and then result G equals await calling G. And that's fine if what you're looking for is more concurrent execution of that part of code, right? So this is, say, in a web method, like a view, to want to make some requests. And there's not a lot you can do to make things go faster, potentially, for that one request. But you can say, let the server be less busy. So it could handle like 10 or 20 times more of the same request. Right. So this real simple, like just await calling these functions, these async functions, it will allow your system to scale more, but it won't make things faster. Like for example, if you're trying to crawl 20 webpages, this will not make it any faster, it'll just make your code more complicated. So don't do that. Right. So there's other ways in which you want to do that. Another thing that I think a lot of people don't quite get is when you call one of these async functions like async def function name, when you call it, it doesn't actually start it until you either await it, or create a task from it. So if you call like F and then G, and you think you're going to come back and get to them later, now they're running. No, actually, they're not unless you've created a task, which starts them. So you either have to await them, which blocks or create these tasks to like kick them off. That's pretty interesting, right? Like, that's not super obvious. I think normally, when you call a function, it does a thing. But here, not so much. So some other options, if you could call them both as create them as tasks. And then you could await those tasks, right, because their tasks are already running. And then you await them both, whichever one first finishes first, doesn't matter, you wait till the first one is done, and maybe the second one is already done. So that's probably the pattern that most people are going to be using. But you can also use async IO gather that takes one or more await doubles as a star arcs. And then it waits for them all to finish, which is pretty cool. And that itself is a future thing that you can await, right. So you would say await async IO gather. Well, that's cool. Yeah, that's awesome. Because I can create all these tasks and say, I just need them all to be done. And when that's done, and we can get the results and carry on and what's cool is when you await gather, you get a tuple of results. So if I say async IO gather function one or task one, task two, then it returns the result one comres or results.

00:05:00 As a tuple, so you can gather them up and get all the answers back, which is pretty cool. Yeah, that's really neat. Yeah, one of the problems with gather, though, is you're saying I'm willing to wait forever for this to finish. And sometimes that's fine. But sometimes things don't return correctly or ever or in the right amount of time. So you can use wait for, which is nice and allows you to pass a timeout. But what's a little bit better than a wait for is there's an async timeout package on pi pi, which I had not heard of. And you can basically create a block that will with block that will timeout. So you can say I want to have an async width, timeout, five seconds, and then do a whole bunch of function calls and awaited and all that. And either they're all going to finish, or when five seconds passes, everything gets canceled, that hasn't finished. That's cool. That's pretty cool, right? Another one that's really interesting is I start a bunch of work. And then I would like to say I kick off, I'm doing web scraping. And I want to go get the results of 20 webpages, I kick off 20 requests. And then I want to process them as they complete, like the first one that's done, I want to work on that then the next one, then the next one. So you can create a task or an iterable, rather from saying async io.as completed and you give it a bunch of tasks. And it gives you an iterator that you can fall in over, that gives you the first completed one, then the second completed one and just blocks until the next one is completed. So you kick a bunch off, and then you just say for completed task in async. io as completed and you give it your running tasks. That's really slick, isn't it? That is slick. Looks like it has a timeout also that you can add to it. Yeah, very nice. Yep, you can give it a timeout. Indeed. Now, there's a few more things covered in there. And I didn't go over the trade offs too much. You know, here's the scenario is this and that. So if you really care about this, two things to do one is check out the article. It's got a lot of details. And each subsection has a little trade offs. Here's the good, here's the bad, which is nice. And also you can check out my async course which talks about this and a whole bunch of other things on async as well. So I'll put a link in the show notes for that as well. Nice. Yeah. So I was talking about waiting. You're talking about what being faster? That sounds better than just waiting around.

00:07:15 Yes, being faster. Well, maybe being faster. Not sure. So I'm still on the fence anyway. So virtual environments, I use virtual environments. Do you use virtual environments, anytime after you install? Any if pip install has to be typed, there's a virtual environment involved? Yeah, yeah, I use it for everything. Even if I've got a machine like a build machine that really only has one Python environment. And I'm only using it for one thing. It's still set up a virtual environment, it's just always, and I've been since the Python three started Python three packaged, V and V. with Python. So you can you can create virtual environments just with the built in V and V package. And I've been using that. Now before that was in there. And if you were in Python two, land, you needed to use the PIP installable, virtual eight, virtual env package. Now it is still updated, and it is still being maintained. And I noticed this was a conversation that started on Twitter this morning, that the virtual environment was still around and it was maybe you should use that. So I went and checked it out, again, the documentation for it. And it says a virtual env is a tool tool to create isolated Python environments. We know this. Since Python 3.3, I guess a subset of has been integrated into the standard library. Yep, the V and V module does not offer all the features of this library, just to name some of the prominent ones, v and v is slower. And it's not extendable. And it cannot create environments with multiple Python versions. And you can't appstate it up data with Pip. It doesn't have a programmatic API. Now, most of that I just really don't care about. But this lower part I do care about. So gave it a shot this morning, I use time, the time function on command line just to time a couple commands, created virtual environments with both V and V in virtual end. And yeah, V and V takes a little over two seconds, two and a half seconds to finish, whereas the virtual m version takes like quite a bit under half a second. So that's, that's a lot. And I mean, if I'm doing a lot of virtual environments, I might care. Now one of the things I was like, coming back and forth, why? Why would I use V and V then a virtual environment, virtual env is faster. Well, you have to pip install virtual em. And so I'll have to remember to do that. I don't think I'll start teaching people this because it's just one more complication thing. And a couple seconds isn't that big of a deal and I still like the process.

00:10:00 To the desk desk prompt, virtual m supports that too, but it handles a different, it doesn't wrap your prompt in parentheses and, and maybe that's just a nicety, but I kind of like it, I'm not sure I'm on the fence as to whether I should switch or, or use it. To me, it feels like, I'm gonna stick with v v. For a long time I saw virtual MV as just like its legacy stuff. It's there. Because before Python three, three, you didn't have the MV built in. So you're going to need it. A lot of the tutorials, talked about it and whatnot. We recently covered it about why it got a big update. And a lot of the things that it does that are nice and the speed is cool to maybe it wouldn't be that hard for to adopt the dash dash prompt dot feature, right? It's open source, right? It could get a PR that does that. Now, that would be pretty cool. Actually, it probably should just so it's consistent. But to me, the idea of having another thing I've got to install somewhere probably into my user profiles, Python packages, so that then I can then create virtual environments so that I can then install things over into that it just, it's fine for me. But as somebody who does courses and teaching and other stuff, like presentations, like it just seems like, okay, you just lost how many people out of that, like, you know, what is the value, like you say it's two seconds. One of the things what I would like to see, and it would be really nice, maybe that would even push me over the edge is he drives me crazy that I create a new virtual environment from the latest version of Python that I can possibly get on the planet. And it tells me that PIP is out of date. Now virtual Verma didn't do that for me this morning. So he created a virtual environment with the newest PIP in Oh, okay, see, now that's pretty nice. Because it's annoying to say, Okay, what you do is create this virtual environment and you pip install this thing, oh, look, there's always going to be a warning. So every single time, what you're going to do is you're going to fix that warning, if I do this, right. So if it grabs the latest, that's actually kind of cool. Now that I think of it, I have a alias in my shell, my startup that I just type V and V and it does the Python dash m, v and v. And then it does an upgrade of Pip. And first it activates it. And then it does an upgrade of PIP and set of tools all in like four characters. So that's what I've been doing these days. Yeah, I've got like a little snippet in my profile also that I'm using. Funny enough, I shared it recently on Twitter, just my like two lines snippet that I used. And then people kept on telling me to use all these other tools. Oh, you could just use this. Like, it's not just use this. It's just a two line snippet and a profile. It's not a big deal. I don't even have to know what it is. I just typed these three characters. I'm good. Why are you bothering me? Right? Why is this such a big deal? I know. It's crazy. Yeah. Anyway, what I would really like to see and none of these address is that something like kind of like Node JS, where it just has the virtual environment at the top level. And it just walks up until it finds a virtual environment, and maybe complains if it doesn't add or does something like that, right? Like something to the effect where you say, You're I know that this is a feature, I forgot what it's called. It's like local Python or something like that. But it's not just built into Python. So if I just went into that directory and tried to run it, it's not going to find and use that version of Python, you know, oh, well, the durin. There's a few. There are a few packages that do that. And that's one of the things that people are directing me to is Darren Dorf is cool. We should talk about that as a separate item. It's worth it. But yeah, Dir E and v is cool. Yeah, yeah, we should talk about that sometime. Yeah, yeah. Good. He's up all our items all on one show, man. Okay, let's thank our sponsor. So Digital Ocean is sponsoring this episode, and Digital Ocean just launched their virtual private cloud and new trust platform. Together. These make it easier to architect and run serious business applications. With even stronger security and confidence. The Virtual Private Cloud or VPC allows you to create multiple private networks for your account or your team instead of having just one private network digitalocean can auto generate your private networks IP address range, or you can specify your own, you can now configure droplets to behave as Internet gateways. And just platform as a new micro site provides you one place to get all your security and privacy questions answered and download their available security certifications. Digital Ocean is your trusted partner in the cloud. Visit Python bytes.fm slash digitalocean to get a $100 credit for new users to build something awesome. Yeah, awesome. Thanks for supporting the show. digitalocean. So before you had to wait on me, Daniel Bryan was Friday. I did have to await this. All right, let me tell you about when you might not even be away Stephanie, things are still slow. So there's this cool analysis done by Chris Whelan's in an article called latency in asynchronous Python, that I don't know if it talks about problems with async IO directly but it

00:15:00 It's more talks about when you have a misconception of how something works over there. And then you apply a couple of patterns or behaviors to it. It might not do what you think. So for probably the best example would be there's, he gives a good one, there's a couple, I'll focus on this, this one word, basically generating too much work and what can be done about it. So he says, I was debugging a program that was having some significant problems, but it was based on async. io. And it would eventually take really long time for like network responses to come back. And it's made of basically two parts. One is this thing that has to send a little heartbeat or receive a heartbeat or something, I don't remember if it's inbound, or outbound. But it has to go beep, beep, beep once every, let's say millisecond, right, so there's an async function, and it just rips through and just every one millisecond, it kicks off one of these heartbeats, totally simple, right? You just say await async, io dot, whatever, like sleep for one millisecond, then do the thing. And then and go on, right, you can basically allow other concurrent work to happen while you're awaiting this sort of like timeout, and to do it on a regular timeframe. And then there's this other stuff that has to do some computational work that takes not very long, like 10 milliseconds. So you're receiving a JSON request, you have to parse that JSON and do like a little bit of work, right? So because async IO runs really on a single thread, that 10 milliseconds is going to block out and stop the heartbeat for 10 milliseconds, which is, you know, whatever, it's fine. It's like, there's some little bit of variability, but it's no big deal. However, if you run a whole bunch of these and his example, Chris said, Let's run 200 of these computational things, and like, just start them up so that they can get put into this queue of work to be done. Well, the way it works is it all gets scheduled, it says, Okay, we have a heartbeat, and we have these 200 little slices of work, each of which is kind of take 10 milliseconds. And there's a bunch of stuff around them. That makes them a little bit slower, the scheduling and whatnot. And then we have a bunch of more heartbeats. So it goes beep, beep, block for two seconds,

00:17:12 beep, beep, beep, where you would expect, okay, I've got all these heartbeats go in, and I've got 200, little async things, let's like mix them up, right, like kind of share it fairly, and it does not do that at all. So it talks about basically what some of the challenges are there. One is, probably you shouldn't just give it that much work. In some giant batch, you should, you know, give it less work at a time, like some kind of like work queue or, you know, he said, let's see if a semaphore can work, I don't remember if semaphores are reentrant or not, it didn't work, the semaphore didn't help at all, actually, so don't use a semaphore to limit to 10. But if semaphores are reentrant, this is all one thread, it doesn't matter, like the semaphore won't block itself. So that's a like this normal threading locking in and stuff like that they kind of don't apply because there's not actual threading going on. So that doesn't really help. But he comes up with this example of the async. io has a job queue, which allows you to push work into it. And then you can like wait for it to be completed. And there's all sorts of cool patterns and like producer consumer stuff that you can put on there. So I actually put together an example, he has like little code snippets, I put together a running example in one program that demonstrates this, and I have a link to the just in the show notes. And I also would like to just point out how much a fan of unsync I am, which I always get, I always talk about when I can around this async stuff like unsync is a library, there's 120 lines of Python and it unifies multi processing async IO, threading, thread, like the all these different API's into like a perfect thing that fits with async and await. It's really, really nice. But applying like the standard unsync adjustments to this code to say like, what you do is just put a decorator at unsync on the function. That's it. You still use await and async and await and all that kind of stuff. The problem is gone. Totally fixes really like you don't have to go to like crazy queues and all that like the problem is gone. It's fixed. Well, it's alleviated. It may still be like if you push it far enough under certain, like, more complex criteria, but the example that showed the problem, you just make them unsink and you await them. It just runs like you would have originally expected like unsink is so beautiful. That's cool. It doesn't change the way async IO works. It basically says okay, the async work is going to run on a background thread. And this other computational stuff will fit into the API, but will technically run on its own thread. So it's it's not like changing the internals but you use the same code. And then Now this doesn't have this problem because the way it slices them together is better I think anyway, it's pretty interesting. It's worth a look. Also have a copy of that on the gist and

00:20:00 You can check that run at two. That's pretty cool. So unsink allows you to possibly not think too much about whether you should have things things just be a sync or whether there should be threads or, yeah, I'm playing it. Is it? Cool? Yeah, it's really neat. I just cleaned everything up. But I sure hope they don't deprecate it, though. Oh, that's a better transition. I was gonna say, thank you. Speaking of cleaning things up, but fireworks do, well just do both transitions. So how to deprecate a pipe package. So you've put up a pipe a package. And for some reason, you don't want it to be up there. I don't want a puppy anymore. Why do I have to take care of that? Yeah. So there's lots of reasons why this might happen. And one of them might just be you accidentally, you didn't use the test API and use the live one. And you put up foo or some variant of foo, and you didn't mean to, maybe it's some other package that somebody took over. And it's handling it better, and you want people to use something else. But anyway, there's lots of reasons why you might, a guy named Paul McCann wrote a blog post about how to deprecate a pipe bi package, so that he gives a few options. And I think these are cool that one of the interesting things is he mentions is the pie doesn't really give you direction as to what it should look like which one you should use. So he's giving his opinion, which is great. deprecate, you might use a deprecation warning. And this doesn't really apply to entire packages. But let's say you've changed your API. So maize will be listed here, it's a good thing to instead of just ripping out parts of your API, leave them in there, but make deprecation warnings in there, they really should be errors instead of warnings if you're or if you're really taking them out. And just having the warning, something like an assert is probably better. But there's a good thing to think about whether don't just rip it out, maybe I don't know. But if you rip it out completely, this cert will happen automatically. So maybe that's a good thing. As far as packages, though, you could just delete it. So you can pipe it does allow you to remove packages, I don't think that that's probably the right thing to do, usually ever, unless you just push something up and it was an accident, then deleting it is fine. But if it's been up there for a while, and people are using it deleting it has a problem that somebody else could take over the name, and possibly a malicious package could take over the name and start to have people having install it. So there's problems with that. So it's probably not a good choice. Most of the time, the last two options are more reasonable. There's a redirect shim. So this is an example. Like let's say, there's an obvious package that is compatible, that is big and better maintained, and you want to push people over to there. If it's really very compatible, you can add a setup a shim, that just there's some code examples here to just, if somebody installs it, it just installs the other package also, that people will give you that one. Yeah.

00:22:56 And even having, if somebody imports your package, it really just imports the other package too. That's a little weird, but it is interesting that that's an option. The thing I really like the probably the best is just a way to fail during install. And there's a code example here for if somebody PIP installs something, and all the packaging works, but the install part will throw a error and you can put a message there redirecting people to use a different package or maybe just explain why you rip this one out. So I think I like the last one best. So most of those are my commentary, but there's some options for how to deal with it so I thought that was good Yeah, I really like the sort of I tried to pip install it and it gives you instead of just failing or being gone it actually gives you a meaningful message like you should use this other package or done if you really intend to leave it that's probably it. Yeah. And one of the interesting things the last couple the redirect shim and the failed during install those he gives example packages that do this. And some of these are just miss type things like if people miss type something try they maybe they meant something else and and to redirecting their Yeah, it seems so right only over at peipsi. And now if you make a mistake, it's not good. So knowing what to do, I mean, people depend on it, right? If you can't get out, then it's trouble. Yeah, but if it's mistake driven, though, make sure you use the test interface first, to play with things before you push garbage up there. There's also I really like people to not squat our names. There's a lot of cool baggage names out there that really have nothing meaningful there because somebody decided they wanted to grab a name and then yeah, didn't do anything with it. That's lame. Don't do that. Yeah, that's definitely lame. On the other hand, there are some times I'm like, how did you just get that name? They'll be like a new package, like secure or something like that. Like how did you get that after all this time? Right. It's It's crazy. Yeah, definitely, yeah, or up to 236,000 packs.

00:25:00 That's pretty insane. Yeah. So Brian, would you like me to enlighten you a little bit and the listeners? Yes, please enlighten me. So last time you brought up a cool progress bar is either the last time or the time before. And it did all sorts of cool stuff. But here's yet another one. Again, an example of our listeners saying, oh, here's three cool things you talked about. But did you know there are these four others you've never heard of yet? So Avraham Lumpkin has sent over his progress bar package called enlightened, and it's actually pretty cool. Like, there's a bunch of cool progress bars with nice animations and stuff. But there's a few features of enlightened that might make you choose it. One is you can have colored progress bars, which is nice. But more importantly, you can have multicolored progress bars. So let me throw out an example that I think would connect for you, given that you're a fan of pi tests, like if you run some sort of series or sequence of operations, and you want to show how far you're making them. But they have multiple outcomes, like Redis, failure, agreement, success, and yellow is like skip or something like that. You could have a progress bar that has three segments, a red segment a, a yellow segment and a green segment. And they could, it could be all one bar, but it could kind of like show you as it grows. Here's the level of failure, here's the level of success and so on, all with color. 224 bit color, not just like eight colors, either. Oh, yeah, that'd be great. Yeah, and then the nice, so there's just go off. The other one is a lot of these progress bars, though, sort of control, they'll be rewriting the screen, right? There'll be putting stuff across as it's happening. But if you happen to like a print statement, effectively, right into standard out or an exception that writes to standard error or something like that, it, you know, messes them all up, right. So this one works, well even allows you to write print statements while it's working. So the print statements kind of low by above it. But it's, you know, whatever part of the screen, it's taken over it's stills managing that as well. So it overrides what print means or standard out. And since it where it belongs. That's cool. Yeah, that's pretty cool. It also automatically handles resizing, except on Windows. And where it says except on Windows, I'm not sure if that means on the new terminal windows terminal that they came out with, it's much closer to what we have over on Mac and Linux. Or if it just means it doesn't work on windows at all. I suspect it it might work on the new windows terminal that just got went 1.0 but certainly not on cmd. Okay, who uses CMD? Okay.

00:27:31 That's what comes with Windows, if you don't like go out of the way to get something right, like commander or the new terminal or something like that. Yeah, yeah. And still get at least on Windows anyway. And then you got bash. Yeah. with it. That's true. So like all good things that have actions and behaviors. And our visual, there's a nice little animation even on the pipe. org page. So if you go there, you can watch and actually see the the stuff scrolling by like an animated GIF right on the IPA page. So very, very nice. Well done, you know, the multi coloured progress bars. It does seem pretty awesome. If that's your use case. I want rainbow ones. I want to do a rainbow. Yes. Maybe with like, little unicorns just shooting out of it. And just like all sorts of crazy yeah. Sounds good. Yes. stars and unicorn. Yes, it would be perfect. Yeah. Let's have that. And people are starting to catch on that we like animations, because they'll included in the suggestion. And by the way, it has an animation because you watch it here. Yeah. Good job bringing it up. Yeah. You're part of it. Awesome. Well, nice work on that progress bar library. It seems simple and well done. Speaking of unicorns, I want to talk about oceans. Wait, unicorns? don't live near Mermaid, mermaid. mermaid. So I want to talk about code ocean. So this was contributed by Daniel Mulkey. So this is a pretty neat thing. So code edition is a paid service. But there's a free tier. And it's a research collaboration platform that supports researchers from the reading of a project through publication. So this is kind of this neat thing. I'm going to read a little bit from their about page, we built a platform that can help give researchers back 20% of the time they spend troubleshooting technology in order to run and reproduce past work before completing new experiments. And code ocean is an open access platform for code and data where users can develop, share, publish and download code through a web browser, eliminating the need to install software, blah, blah, blah. Anyway, mission is to make research easier. So this idea is you can have like code snippets like Jupiter, and Python and even things like MATLAB and c++ code, running with the data in this kind of environment that you can collaborate with other people and just sort of build up these data sets and the science and the code and all bundled together. And it's pretty cool. It also collaborates with some one

00:30:00 The goals of it is to be able to have all of this reproducible code and data together in a form that's acceptable to to journals. And one of the reasons why it was contributed, as Daniel said that one of the peer reviewed journals that he reads, it happened to be SP is optical engineering journal, recommended this platform for associating code with the article. So people trying to be do science and be published, associating a code ocean space, with it is an option. That's cool. And if it gets accepted by editors as Yeah, that's what you do, then it just makes it easier to like, it's kind of like saying, Oh, you have an open source project. What's the GitHub URL? Right not? Is it on GitHub? Just where I'm gonna visit? Yeah, I do technically know that there's Git lab and other places, but like most of the code lives on GitHub is what I was getting. Right.

00:30:51 Yeah, cool. Well, this looks pretty neat. I do think there's a lot of interesting takes on reproducibility in science. And that's definitely a good thing. There's this. There's binder, which is doing a lot of interesting stuff, although not as focused on exact reproducibility, but still nice. There's gigantism, which is also a cool platform for this kind of stuff. So there's a lot of options, and it's nice to see more of my code ocean. Yeah. Nice. Well, that's our six. Do you have anything extra for us today? Ah, I'm gonna try to connect this to Python, because I want to be excited because I am excited about it. How long has it been since astronauts have been launched into space in from from NASA and from the US, like, it's been a really long time ever since the space shuttle got shut down four or five years ago, I heard it was like over 10. But I'm kind of her drone. It's very possible. It's been a very long time. So today, I know this doesn't help you folks listening, because the time it takes us to get this upset out. But hopefully this went well. But I'm super excited for Space X is launched in collaboration with NASA to send two astronauts up into space. Wow, are those guys brave to get on to one of these rockets.

00:32:04 But also, I think there's probably somewhere in the mix a lot of Python in action. If you go to SpaceX, they had last time I looked at one random point a couple months ago, they had 92 open positions for Python developers. Oh, wow. I don't know if that's 92 people they're looking for, but at least there's 92 roles they were trying to fill, there could be multiple people into any one role. So that's a lot of Python. And so somehow in this launch, there's got to be some interesting stories around Python. And this is mostly to say, one, it's awesome that SpaceX and NASA are doing this. Hopefully this goes well. Lots of lots of luck to that. But also, if anyone knows how to connect us with the people inside SpaceX doing awesome rocket stuff with Python, those would make great stories that we would love to hear about those introductions. Yeah, that'd be cool. I'd love to hear more about that. Yeah, I do hope because well, later, and I heard that possibly there was weather problems that might crop up. But well, maybe people will get to watch this.

00:33:08 delayed for a week, we'll see.

00:33:10 Awesome, how about you? What do you got? I just downloaded 3.90 beta one. So Python three, nine, beta one is available for testing. If you are maintaining a package or any other maintaining your application, you probably have to download and make sure your stuff works with 390. Yeah, that's cool. And because it's a beta now it should be frozen. In terms of features and API's and stuff, right? It's no longer changing. So it's now time to start making sure your stuff works and yelling if it Yeah, right. Another reason to download it is the prompt, virtual V and V with the prompt with the magic dot that turns.

00:33:49 Yeah, yeah. That is in three, nine. Yeah, super cool. Awesome. Well, that's not very funny. But I could tell you something that is, and it's very relevant to your item here, actually. Okay, you're ready for this. So open up this, this link here, and I'll put the link in the show notes. Because this is a visual, I gotta describe it to you. So this was sent over by Steven how thank you for that. And this would be better during Halloween. But Halloween is far away. So we're going to do it this way. So there's a person standing around and there's a ghost standing behind them. Right? Yeah, it goes says blue person doesn't react blue person, that blue person doesn't react. The person says Python to seven. Ah, the person runs away.

00:34:31 Yeah, this is great. Let's get right. Yeah, you got one as well. What do you got here? Well, I'm gonna get haters for this, but I'm gonna say it anyway. So somebody named Bert said does a meta joke. Because we have used pi jokes before. We love pi jokes. And I'm going to modify it a little bit. So what does pi jokes have in common with Java? Oh, it gets updated all the time, but never gets any better.

00:34:55 That's pretty fun. I don't even really use Java but I have a Java tool on my

00:35:00 desktop. And so I get like Java's updated. Do you want to do the update all the time? Make it stop? Yeah. Yeah, that's funny. Yeah, pie jokes is good. If y'all need some programming jokes, just pip install dash s user pie jokes. And then you can type by joke. anytime you want. Yeah, I had to change it. Because the original joke was like about flash, Adobe Flash. And who has that? Is that even a thing anymore? Yeah, I don't even think it gets updated anymore. I don't know. Maybe it does. I sure hope it's not on my computer. Yeah.

00:35:32 Yeah, it totally is. Awesome. All right. Well, very funny. All right. Thank you. Yep, you bet. Bye bye. Thank you for listening to Python bytes. Follow the show on twitter at Python bytes. That's Python bytes as in V yts. And get the full show notes at Python bytes at FM. If you have a news item you want featured, just visit Python bytes.fm and send it our way. We're always on the lookout for sharing something cool. This is Brian Aachen and on behalf of myself and Michael Kennedy, thank you for listening and sharing this podcast with your friends and colleagues.

